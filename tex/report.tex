\documentclass[a4paper,12pt]{article}


\usepackage{amssymb,amsmath,array}
\usepackage{hyperref}
\usepackage{bm}

\usepackage[a4paper, left=3cm, right=2.5cm, top=3cm, bottom=3cm]{geometry}
% language and encoding
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
% this package load language specific quotes and enables you to write
% \enquote{text} instead of "`text"' or something like that.
% \usepackage{csquotes}
\newcommand{\ts}{\textsuperscript}

% Write initials inside the right margin
\newcommand{\initials}[1]{\marginpar{\quad\texttt{#1}}}

\title{Report of the 3\ts{rd} exercise sheet}
\author{Marco Adamczyk (MA) \and Till Brinkmann (TB)}

% could removed
\date{Submission by 06\ts{th} December 2019}


\begin{document}

\pagenumbering{gobble}

\pagestyle{myheadings}
\markright{Marco Adamczyk, Till Brinkmann}
    
\maketitle

\begin{center}
    \textbf{Tutorial: Tuesday, Tutor: Riza Velioglu}
\end{center}

\section{Introduction}
This is an template for a report. You may use the outline or change it freely. There are no directives.
\subsection{Datasets}
"dataset1"


\section{Methods/Models}
Which classifiers are used? How do they classify/differ?

\subsection{k-nearest-Neighbor classifier}
The knn classifier represent the data on a n-dimational space and in this space the distance of dataset element to the new element is calculated by a specific metric. In the given case, the p-Norm with p = 1 is used, because the dataset vector entries are zero or one, so the relative distance will stay the same.
After the distance calculation, the k closest dataset points to the new element are chosen. Now the new data has to be classified by those selected dataset points. There are multiple approaches. The naive apporach would be to select the class that appears on the selected data most often. More ideas are disscussed below.
\initials{MA}
\subsection{Classification tree}
The classfier structures the trainingsdata as a tree, so the 
\initials{MA}
\section{Bayes Experiment}

\section{kNN Experiment}
The knn-algorithm was implemented as a class in python. A method was saving the training set and its outputs in two variables. With the second method "predict(element,k)" the estimated class for a new element was returned. In short, it calculated the distance in a help method for every data in the trainings set. Then it iterates through that list and selects the k-nearest elements. Form the selected elements the class is chosen that appears the most in these elements.

\initials{MA}
\subsection{Data}
The two given datasets were used to test the classifier. The datasets were split in two parts. The testset contains 20 elements from the data set. The other data is used as the trainingset. Every data from the testset will be classified for a k = 1..10. As seen in the graphics bellow, there is not significant difference between the balanced and unbalanced dataset. The kNN only classifies with selected element not the whole dataset, so it is only the dependent how the classes in the dataset are distributed.

\initials{MA}
\subsection{Results}

\begin{figure}[h]
\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
\caption{Results}
\label{fig_res}
\end{figure}


You can refer to Figure~\ref{fig_res}.
\begin{table}[h]
\caption{An Example of a Table}
\label{tab_example}
\centering
% Some packages, such as MDW tools, offer better commands for making tables
% than the plain LaTeX2e tabular which is used here.
\begin{tabular}{c||c|c|c}
Data & Method 1 & Method 2 & Method 3\\
\hline\hline
data 1 &0.54 & 0.6& 0.98\\
\hline
data 2 &0.74 & 0.54& 0.48\\
\hline
data 3 &0.82 & 0.71& 0.67
\end{tabular}
\end{table}
You can also refer to Table~\ref{tab_example}.
\initials{FGM}

\section{Discussion}
As seen in the results some classifiers are more suited for specific tasks.
\initials{TB/MA}

\section{Further Features}
\subsection{Data Features}
Treegramms
\initials{TB}
\subsection{kNN Features}
The kNNs selection of the class only the depends on the number of occurences of a certain class. It could be reasonable to include the distance, because the following problem could occur:
Assume k = 4. 3 elements are from the class 1 and 1 element is from the class 0. Normally, the data would be classified as 1, but if the distances of the class 1 elements are far away and the element with class 0 is a lot closer it could be more reasonable to classify the new data as class 0.
To include the distance as a new parameter to the calculation, the average distance from the new data and a specific class could be calculated and the compared. With this change, we get the following result for the same setup presented in the results above:
\begin{figure}[h]
\centering
%\includegraphics[width=2.5in]{myfigure}
% where an .eps filename suffix will be assumed under latex, 
% and a .pdf suffix will be assumed for pdflatex; or what has been declared
% via \DeclareGraphicsExtensions.
\caption{Results}
\label{fig_res}
\end{figure}


You can refer to Figure~\ref{fig_res}.
\begin{table}[h]
\caption{An Example of a Table}
\label{tab_example}
\centering
% Some packages, such as MDW tools, offer better commands for making tables
% than the plain LaTeX2e tabular which is used here.
\begin{tabular}{c||c|c|c}
Data & Method 1 & Method 2 & Method 3\\
\hline\hline
data 1 &0.54 & 0.6& 0.98\\
\hline
data 2 &0.74 & 0.54& 0.48\\
\hline
data 3 &0.82 & 0.71& 0.67
\end{tabular}
\end{table}

\initials{MA}  
\end{document}